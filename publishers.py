#!/usr/bin/env python3
"""
Data Publishers for OPC UA Server
Supports publishing tag data to multiple protocols: MQTT, REST API, WebSockets, etc.

Author: Your Friendly Neighborhood Engineer
License: MIT
"""

import json
import logging
import threading
import time
import requests  # For HTTP requests (Slack webhooks, etc.)
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import paho.mqtt.client as mqtt
from flask import Flask, jsonify, request
from flask_cors import CORS

try:
    from sparkplug_b import *
    SPARKPLUG_AVAILABLE = True
except ImportError:
    SPARKPLUG_AVAILABLE = False

try:
    from kafka import KafkaProducer
    KAFKA_AVAILABLE = True
except ImportError:
    KAFKA_AVAILABLE = False

try:
    import pika
    AMQP_AVAILABLE = True
except ImportError:
    AMQP_AVAILABLE = False

try:
    from websocket_server import WebsocketServer
    WEBSOCKET_AVAILABLE = True
except ImportError:
    WEBSOCKET_AVAILABLE = False

try:
    from pymodbus.server import StartTcpServer
    from pymodbus.datastore import ModbusSequentialDataBlock, ModbusSlaveContext, ModbusServerContext
    from pymodbus.device import ModbusDeviceIdentification
    import struct
    MODBUS_AVAILABLE = True
except ImportError:
    MODBUS_AVAILABLE = False

try:
    from opcua import Client as OPCUAClient
    from opcua import ua
    OPCUA_CLIENT_AVAILABLE = True
except ImportError:
    OPCUA_CLIENT_AVAILABLE = False

try:
    import graphene
    from flask_graphql import GraphQLView
    GRAPHQL_AVAILABLE = True
except ImportError:
    GRAPHQL_AVAILABLE = False

try:
    from influxdb_client import InfluxDBClient, Point, WritePrecision
    from influxdb_client.client.write_api import SYNCHRONOUS
    INFLUXDB_AVAILABLE = True
except ImportError:
    INFLUXDB_AVAILABLE = False

# Email and notification libraries (mostly built-in)
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timedelta
from collections import deque

try:
    from twilio.rest import Client as TwilioClient
    TWILIO_AVAILABLE = True
except ImportError:
    TWILIO_AVAILABLE = False


class DataPublisher(ABC):
    """Base class for all data publishers."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialize the data publisher.
        
        Args:
            config: Publisher-specific configuration
            logger: Logger instance
        """
        self.config = config
        self.enabled = config.get("enabled", False)
        self.logger = logger or logging.getLogger(self.__class__.__name__)
        self.running = False
        
    @abstractmethod
    def start(self):
        """Start the publisher."""
        pass
    
    @abstractmethod
    def stop(self):
        """Stop the publisher."""
        pass
    
    @abstractmethod
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish a tag value.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        pass


class MQTTPublisher(DataPublisher):
    """MQTT Publisher for tag data."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        self.client = None
        self.connected = False
        
    def on_connect(self, client, userdata, flags, rc):
        """Callback for when the client receives a CONNACK response."""
        if rc == 0:
            self.connected = True
            self.logger.info("Connected to MQTT broker successfully")
            
            # Subscribe to command topics if configured
            command_topic = self.config.get("command_topic")
            if command_topic:
                client.subscribe(f"{command_topic}/#")
                self.logger.info(f"Subscribed to command topic: {command_topic}/#")
        else:
            self.logger.error(f"Failed to connect to MQTT broker, return code {rc}")
            self.connected = False
    
    def on_disconnect(self, client, userdata, rc):
        """Callback for when the client disconnects."""
        self.connected = False
        if rc != 0:
            self.logger.warning(f"Unexpected MQTT disconnection (rc={rc}). Attempting to reconnect...")
    
    def on_message(self, client, userdata, msg):
        """Callback for when a PUBLISH message is received."""
        try:
            topic = msg.topic
            payload = msg.payload.decode()
            self.logger.info(f"Received MQTT message on {topic}: {payload}")
            
            # Parse the command (could be used for write-back to OPC UA)
            # Format: command_topic/tag_name -> value
            if hasattr(self, 'command_callback'):
                tag_name = topic.split('/')[-1]
                self.command_callback(tag_name, payload)
                
        except Exception as e:
            self.logger.error(f"Error processing MQTT message: {e}")
    
    def start(self):
        """Start the MQTT publisher."""
        if not self.enabled:
            self.logger.info("MQTT publisher is disabled")
            return
        
        try:
            broker = self.config.get("broker", "localhost")
            port = self.config.get("port", 1883)
            client_id = self.config.get("client_id", "opcua_server")
            
            self.client = mqtt.Client(client_id=client_id)
            self.client.on_connect = self.on_connect
            self.client.on_disconnect = self.on_disconnect
            self.client.on_message = self.on_message
            
            # Set username/password if provided
            username = self.config.get("username")
            password = self.config.get("password")
            if username and password:
                self.client.username_pw_set(username, password)
            
            # Configure TLS if specified
            use_tls = self.config.get("use_tls", False)
            if use_tls:
                ca_certs = self.config.get("ca_certs")
                self.client.tls_set(ca_certs=ca_certs)
            
            self.logger.info(f"Connecting to MQTT broker at {broker}:{port}")
            self.client.connect(broker, port, keepalive=60)
            self.client.loop_start()
            self.running = True
            
        except Exception as e:
            self.logger.error(f"Failed to start MQTT publisher: {e}")
            self.running = False
    
    def stop(self):
        """Stop the MQTT publisher."""
        if self.client and self.running:
            self.client.loop_stop()
            self.client.disconnect()
            self.running = False
            self.logger.info("MQTT publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish tag value to MQTT.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not self.connected:
            return
        
        try:
            topic_prefix = self.config.get("topic_prefix", "opcua")
            topic = f"{topic_prefix}/{tag_name}"
            
            # Create payload
            payload_format = self.config.get("payload_format", "json")
            
            if payload_format == "json":
                payload_data = {
                    "tag": tag_name,
                    "value": value,
                    "timestamp": timestamp or time.time()
                }
                payload = json.dumps(payload_data)
            else:
                # Simple string format
                payload = str(value)
            
            qos = self.config.get("qos", 0)
            retain = self.config.get("retain", False)
            
            self.client.publish(topic, payload, qos=qos, retain=retain)
            self.logger.debug(f"Published to MQTT: {topic} = {payload}")
            
        except Exception as e:
            self.logger.error(f"Error publishing to MQTT: {e}")
    
    def set_command_callback(self, callback):
        """Set callback function for handling incoming commands."""
        self.command_callback = callback


class RESTAPIPublisher(DataPublisher):
    """REST API Publisher for tag data."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        self.app = Flask(__name__)
        CORS(self.app)
        self.server_thread = None
        self.tag_cache = {}
        self.write_callback = None
        
        # Setup routes
        self.setup_routes()
    
    def setup_routes(self):
        """Setup Flask routes."""
        
        @self.app.route('/api/tags', methods=['GET'])
        def get_all_tags():
            """Get all tag values."""
            return jsonify({
                "tags": self.tag_cache,
                "count": len(self.tag_cache)
            })
        
        @self.app.route('/api/tags/<tag_name>', methods=['GET'])
        def get_tag(tag_name):
            """Get a specific tag value."""
            if tag_name in self.tag_cache:
                return jsonify(self.tag_cache[tag_name])
            return jsonify({"error": "Tag not found"}), 404
        
        @self.app.route('/api/tags/<tag_name>', methods=['POST', 'PUT'])
        def write_tag(tag_name):
            """Write a value to a tag."""
            try:
                data = request.get_json()
                value = data.get('value')
                
                if value is None:
                    return jsonify({"error": "No value provided"}), 400
                
                # Call write callback if set
                if self.write_callback:
                    success = self.write_callback(tag_name, value)
                    if success:
                        return jsonify({"success": True, "tag": tag_name, "value": value})
                    else:
                        return jsonify({"error": "Failed to write tag"}), 500
                        
                return jsonify({"error": "Write not supported"}), 501
                
            except Exception as e:
                self.logger.error(f"Error writing tag via API: {e}")
                return jsonify({"error": str(e)}), 500
        
        @self.app.route('/api/health', methods=['GET'])
        def health_check():
            """Health check endpoint."""
            return jsonify({
                "status": "healthy",
                "tags_count": len(self.tag_cache)
            })
    
    def start(self):
        """Start the REST API server."""
        if not self.enabled:
            self.logger.info("REST API publisher is disabled")
            return
        
        try:
            host = self.config.get("host", "0.0.0.0")
            port = self.config.get("port", 5000)
            
            def run_server():
                self.app.run(host=host, port=port, debug=False, use_reloader=False)
            
            self.server_thread = threading.Thread(target=run_server, daemon=True)
            self.server_thread.start()
            self.running = True
            
            self.logger.info(f"REST API started on http://{host}:{port}")
            
        except Exception as e:
            self.logger.error(f"Failed to start REST API: {e}")
            self.running = False
    
    def stop(self):
        """Stop the REST API server."""
        # Flask doesn't have a clean shutdown in this mode
        # In production, use a proper WSGI server
        self.running = False
        self.logger.info("REST API publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Update tag cache for REST API.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled:
            return
        
        self.tag_cache[tag_name] = {
            "value": value,
            "timestamp": timestamp or time.time()
        }
    
    def set_write_callback(self, callback):
        """Set callback function for handling write requests."""
        self.write_callback = callback


class SparkplugBPublisher(DataPublisher):
    """Sparkplug B Publisher for Ignition Edge and SCADA systems."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        if not SPARKPLUG_AVAILABLE:
            self.logger.warning("Sparkplug B library not available. Install with: pip install sparkplug-b")
            self.enabled = False
            return
            
        self.client = None
        self.connected = False
        self.sequence_number = 0
        self.bdSeq = 0
        
    def get_next_sequence(self):
        """Get next sequence number (0-255)."""
        seq = self.sequence_number
        self.sequence_number = (self.sequence_number + 1) % 256
        return seq
    
    def on_connect(self, client, userdata, flags, rc):
        """Callback for when the client receives a CONNACK response."""
        if rc == 0:
            self.connected = True
            self.logger.info("Connected to Sparkplug B broker successfully")
            
            # Send NBIRTH (Node Birth) message
            self.send_node_birth()
            # Send DBIRTH (Device Birth) message
            self.send_device_birth()
        else:
            self.logger.error(f"Failed to connect to Sparkplug B broker, return code {rc}")
            self.connected = False
    
    def on_disconnect(self, client, userdata, rc):
        """Callback for when the client disconnects."""
        self.connected = False
        if rc != 0:
            self.logger.warning(f"Unexpected Sparkplug B disconnection (rc={rc})")
    
    def send_node_birth(self):
        """Send Node Birth Certificate."""
        if not self.connected:
            return
            
        try:
            group_id = self.config.get("group_id", "Sparkplug B Devices")
            edge_node_id = self.config.get("edge_node_id", "OPC_UA_Gateway")
            
            topic = f"spBv1.0/{group_id}/NBIRTH/{edge_node_id}"
            
            # Create minimal NBIRTH payload
            payload = {
                "timestamp": int(time.time() * 1000),
                "metrics": [
                    {
                        "name": "Node Control/Rebirth",
                        "timestamp": int(time.time() * 1000),
                        "dataType": "Boolean",
                        "value": False
                    }
                ],
                "seq": self.get_next_sequence()
            }
            
            self.client.publish(topic, json.dumps(payload), qos=0, retain=False)
            self.logger.info(f"Sent NBIRTH to {topic}")
            
        except Exception as e:
            self.logger.error(f"Error sending NBIRTH: {e}")
    
    def send_device_birth(self):
        """Send Device Birth Certificate with all metrics."""
        if not self.connected:
            return
            
        try:
            group_id = self.config.get("group_id", "Sparkplug B Devices")
            edge_node_id = self.config.get("edge_node_id", "OPC_UA_Gateway")
            device_id = self.config.get("device_id", "EdgeDevice")
            
            topic = f"spBv1.0/{group_id}/DBIRTH/{edge_node_id}/{device_id}"
            
            # Create DBIRTH payload with metrics
            payload = {
                "timestamp": int(time.time() * 1000),
                "metrics": [],
                "seq": self.get_next_sequence()
            }
            
            self.client.publish(topic, json.dumps(payload), qos=0, retain=False)
            self.logger.info(f"Sent DBIRTH to {topic}")
            
        except Exception as e:
            self.logger.error(f"Error sending DBIRTH: {e}")
    
    def start(self):
        """Start the Sparkplug B publisher."""
        if not self.enabled or not SPARKPLUG_AVAILABLE:
            if not SPARKPLUG_AVAILABLE:
                self.logger.warning("Sparkplug B publisher is disabled (library not available)")
            else:
                self.logger.info("Sparkplug B publisher is disabled")
            return
        
        try:
            broker = self.config.get("broker", "localhost")
            port = self.config.get("port", 1883)
            group_id = self.config.get("group_id", "Sparkplug B Devices")
            edge_node_id = self.config.get("edge_node_id", "OPC_UA_Gateway")
            
            client_id = f"{group_id}_{edge_node_id}"
            
            self.client = mqtt.Client(client_id=client_id)
            self.client.on_connect = self.on_connect
            self.client.on_disconnect = self.on_disconnect
            
            # Set username/password if provided
            username = self.config.get("username")
            password = self.config.get("password")
            if username and password:
                self.client.username_pw_set(username, password)
            
            # Configure NDEATH (Node Death) certificate as LWT
            ndeath_topic = f"spBv1.0/{group_id}/NDEATH/{edge_node_id}"
            ndeath_payload = {
                "timestamp": int(time.time() * 1000),
                "bdSeq": self.bdSeq
            }
            self.client.will_set(ndeath_topic, json.dumps(ndeath_payload), qos=0, retain=False)
            
            self.logger.info(f"Connecting to Sparkplug B broker at {broker}:{port}")
            self.client.connect(broker, port, keepalive=60)
            self.client.loop_start()
            self.running = True
            
        except Exception as e:
            self.logger.error(f"Failed to start Sparkplug B publisher: {e}")
            self.running = False
    
    def stop(self):
        """Stop the Sparkplug B publisher."""
        if self.client and self.running:
            # Send NDEATH before disconnecting
            try:
                group_id = self.config.get("group_id", "Sparkplug B Devices")
                edge_node_id = self.config.get("edge_node_id", "OPC_UA_Gateway")
                
                topic = f"spBv1.0/{group_id}/NDEATH/{edge_node_id}"
                payload = {
                    "timestamp": int(time.time() * 1000),
                    "bdSeq": self.bdSeq
                }
                self.client.publish(topic, json.dumps(payload), qos=0, retain=False)
            except:
                pass
                
            self.client.loop_stop()
            self.client.disconnect()
            self.running = False
            self.logger.info("Sparkplug B publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish tag value using Sparkplug B DDATA message.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not self.connected or not SPARKPLUG_AVAILABLE:
            return
        
        try:
            group_id = self.config.get("group_id", "Sparkplug B Devices")
            edge_node_id = self.config.get("edge_node_id", "OPC_UA_Gateway")
            device_id = self.config.get("device_id", "EdgeDevice")
            
            topic = f"spBv1.0/{group_id}/DDATA/{edge_node_id}/{device_id}"
            
            # Determine Sparkplug data type
            if isinstance(value, bool):
                datatype = "Boolean"
            elif isinstance(value, int):
                datatype = "Int32"
            elif isinstance(value, float):
                datatype = "Float"
            elif isinstance(value, str):
                datatype = "String"
            else:
                datatype = "String"
                value = str(value)
            
            # Create DDATA payload
            payload = {
                "timestamp": int((timestamp or time.time()) * 1000),
                "metrics": [
                    {
                        "name": tag_name,
                        "timestamp": int((timestamp or time.time()) * 1000),
                        "dataType": datatype,
                        "value": value
                    }
                ],
                "seq": self.get_next_sequence()
            }
            
            self.client.publish(topic, json.dumps(payload), qos=0, retain=False)
            self.logger.debug(f"Published Sparkplug B DDATA: {tag_name} = {value}")
            
        except Exception as e:
            self.logger.error(f"Error publishing to Sparkplug B: {e}")


class KafkaPublisher(DataPublisher):
    """Apache Kafka Publisher for enterprise streaming."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        if not KAFKA_AVAILABLE:
            self.logger.warning("Kafka library not available. Install with: pip install kafka-python")
            self.enabled = False
            return
            
        self.producer = None
        
    def start(self):
        """Start the Kafka publisher."""
        if not self.enabled or not KAFKA_AVAILABLE:
            if not KAFKA_AVAILABLE:
                self.logger.warning("Kafka publisher is disabled (library not available)")
            else:
                self.logger.info("Kafka publisher is disabled")
            return
        
        try:
            bootstrap_servers = self.config.get("bootstrap_servers", ["localhost:9092"])
            
            # Convert string to list if needed
            if isinstance(bootstrap_servers, str):
                bootstrap_servers = [bootstrap_servers]
            
            self.producer = KafkaProducer(
                bootstrap_servers=bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                compression_type=self.config.get("compression", "gzip")
            )
            
            self.running = True
            self.logger.info(f"Kafka publisher started with brokers: {bootstrap_servers}")
            
        except Exception as e:
            self.logger.error(f"Failed to start Kafka publisher: {e}")
            self.running = False
    
    def stop(self):
        """Stop the Kafka publisher."""
        if self.producer and self.running:
            self.producer.flush()
            self.producer.close()
            self.running = False
            self.logger.info("Kafka publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish tag value to Kafka topic.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not self.running or not KAFKA_AVAILABLE:
            return
        
        try:
            topic = self.config.get("topic", "industrial-data")
            
            # Create message payload
            message = {
                "tag": tag_name,
                "value": value,
                "timestamp": timestamp or time.time()
            }
            
            # Include tag name as key for partitioning
            key = tag_name.encode('utf-8')
            
            self.producer.send(topic, value=message, key=key)
            self.logger.debug(f"Published to Kafka: {tag_name} = {value}")
            
        except Exception as e:
            self.logger.error(f"Error publishing to Kafka: {e}")


class AMQPPublisher(DataPublisher):
    """AMQP Publisher for RabbitMQ and other AMQP brokers."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        if not AMQP_AVAILABLE:
            self.logger.warning("AMQP library not available. Install with: pip install pika")
            self.enabled = False
            return
            
        self.connection = None
        self.channel = None
        
    def start(self):
        """Start the AMQP publisher."""
        if not self.enabled or not AMQP_AVAILABLE:
            if not AMQP_AVAILABLE:
                self.logger.warning("AMQP publisher is disabled (library not available)")
            else:
                self.logger.info("AMQP publisher is disabled")
            return
        
        try:
            host = self.config.get("host", "localhost")
            port = self.config.get("port", 5672)
            username = self.config.get("username", "guest")
            password = self.config.get("password", "guest")
            vhost = self.config.get("virtual_host", "/")
            
            credentials = pika.PlainCredentials(username, password)
            parameters = pika.ConnectionParameters(
                host=host,
                port=port,
                virtual_host=vhost,
                credentials=credentials
            )
            
            self.connection = pika.BlockingConnection(parameters)
            self.channel = self.connection.channel()
            
            # Declare exchange
            exchange = self.config.get("exchange", "industrial.data")
            exchange_type = self.config.get("exchange_type", "topic")
            self.channel.exchange_declare(
                exchange=exchange,
                exchange_type=exchange_type,
                durable=True
            )
            
            self.running = True
            self.logger.info(f"AMQP publisher started on {host}:{port}")
            
        except Exception as e:
            self.logger.error(f"Failed to start AMQP publisher: {e}")
            self.running = False
    
    def stop(self):
        """Stop the AMQP publisher."""
        if self.connection and self.running:
            try:
                self.channel.close()
                self.connection.close()
            except:
                pass
            self.running = False
            self.logger.info("AMQP publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish tag value to AMQP exchange.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not self.running or not AMQP_AVAILABLE:
            return
        
        try:
            exchange = self.config.get("exchange", "industrial.data")
            routing_key = self.config.get("routing_key_prefix", "opcua") + "." + tag_name
            
            # Create message payload
            message = {
                "tag": tag_name,
                "value": value,
                "timestamp": timestamp or time.time()
            }
            
            self.channel.basic_publish(
                exchange=exchange,
                routing_key=routing_key,
                body=json.dumps(message),
                properties=pika.BasicProperties(
                    delivery_mode=2,  # Make message persistent
                    content_type='application/json'
                )
            )
            
            self.logger.debug(f"Published to AMQP: {routing_key} = {value}")
            
        except Exception as e:
            self.logger.error(f"Error publishing to AMQP: {e}")


class WebSocketPublisher(DataPublisher):
    """WebSocket Publisher for real-time browser updates."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        if not WEBSOCKET_AVAILABLE:
            self.logger.warning("WebSocket library not available. Install with: pip install websocket-server")
            self.enabled = False
            return
            
        self.server = None
        self.server_thread = None
        self.clients = []
        
    def new_client(self, client, server):
        """Called when a new client connects."""
        self.clients.append(client)
        self.logger.info(f"New WebSocket client connected: {client['id']}")
    
    def client_left(self, client, server):
        """Called when a client disconnects."""
        if client in self.clients:
            self.clients.remove(client)
        self.logger.info(f"WebSocket client disconnected: {client['id']}")
    
    def start(self):
        """Start the WebSocket server."""
        if not self.enabled or not WEBSOCKET_AVAILABLE:
            if not WEBSOCKET_AVAILABLE:
                self.logger.warning("WebSocket publisher is disabled (library not available)")
            else:
                self.logger.info("WebSocket publisher is disabled")
            return
        
        try:
            host = self.config.get("host", "0.0.0.0")
            port = self.config.get("port", 9001)
            
            self.server = WebsocketServer(host=host, port=port)
            self.server.set_fn_new_client(self.new_client)
            self.server.set_fn_client_left(self.client_left)
            
            def run_server():
                self.server.run_forever()
            
            self.server_thread = threading.Thread(target=run_server, daemon=True)
            self.server_thread.start()
            self.running = True
            
            self.logger.info(f"WebSocket server started on ws://{host}:{port}")
            
        except Exception as e:
            self.logger.error(f"Failed to start WebSocket publisher: {e}")
            self.running = False
    
    def stop(self):
        """Stop the WebSocket server."""
        if self.server and self.running:
            self.server.shutdown()
            self.running = False
            self.logger.info("WebSocket publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Broadcast tag value to all connected WebSocket clients.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not self.running or not WEBSOCKET_AVAILABLE:
            return
        
        try:
            if not self.clients:
                return
            
            # Create message payload
            message = {
                "tag": tag_name,
                "value": value,
                "timestamp": timestamp or time.time()
            }
            
            message_json = json.dumps(message)
            
            # Broadcast to all connected clients
            for client in self.clients[:]:  # Use copy to avoid modification during iteration
                try:
                    self.server.send_message(client, message_json)
                except:
                    # Remove disconnected clients
                    if client in self.clients:
                        self.clients.remove(client)
            
            self.logger.debug(f"Broadcast to {len(self.clients)} WebSocket clients: {tag_name} = {value}")
            
        except Exception as e:
            self.logger.error(f"Error publishing to WebSocket: {e}")


class ModbusTCPPublisher(DataPublisher):
    """MODBUS TCP Server Publisher for legacy industrial systems."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        if not MODBUS_AVAILABLE:
            self.logger.warning("MODBUS library not available. Install with: pip install pymodbus")
            self.enabled = False
            return
            
        self.server_thread = None
        self.context = None
        self.tag_register_map = {}  # Maps tag names to register addresses
        self.register_tag_map = {}  # Maps register addresses to tag info
        self.next_register = 0
        
    def allocate_registers(self, tag_name: str, tag_type: str) -> int:
        """
        Allocate MODBUS registers for a tag.
        
        Args:
            tag_name: Name of the tag
            tag_type: Type of tag (int, float, bool, string)
            
        Returns:
            Starting register address
        """
        if tag_name in self.tag_register_map:
            return self.tag_register_map[tag_name]["start_register"]
        
        # Determine number of registers needed
        if tag_type == "float":
            num_registers = 2  # 32-bit float = 2 registers
        elif tag_type == "int":
            num_registers = 1  # 16-bit int = 1 register
        elif tag_type == "bool":
            num_registers = 1  # Boolean = 1 register (0 or 1)
        elif tag_type == "string":
            num_registers = 32  # Reserve 32 registers (64 bytes) for strings
        else:
            num_registers = 1
        
        start_register = self.next_register
        self.tag_register_map[tag_name] = {
            "start_register": start_register,
            "num_registers": num_registers,
            "type": tag_type
        }
        
        # Create reverse mapping for reading
        for i in range(num_registers):
            self.register_tag_map[start_register + i] = {
                "tag_name": tag_name,
                "offset": i,
                "type": tag_type
            }
        
        self.next_register += num_registers
        self.logger.debug(f"Allocated registers {start_register}-{start_register + num_registers - 1} for {tag_name} ({tag_type})")
        
        return start_register
    
    def start(self):
        """Start the MODBUS TCP server."""
        if not self.enabled or not MODBUS_AVAILABLE:
            if not MODBUS_AVAILABLE:
                self.logger.warning("MODBUS TCP publisher is disabled (library not available)")
            else:
                self.logger.info("MODBUS TCP publisher is disabled")
            return
        
        try:
            host = self.config.get("host", "0.0.0.0")
            port = self.config.get("port", 502)
            
            # Initialize register mapping from config
            register_mapping = self.config.get("register_mapping", {})
            for tag_name, mapping in register_mapping.items():
                tag_type = mapping.get("type", "float")
                if "register" in mapping:
                    # Use explicit register assignment
                    start_reg = mapping["register"]
                    num_regs = 2 if tag_type == "float" else 1
                    self.tag_register_map[tag_name] = {
                        "start_register": start_reg,
                        "num_registers": num_regs,
                        "type": tag_type
                    }
                    self.next_register = max(self.next_register, start_reg + num_regs)
            
            # Create MODBUS datastore (65536 registers, initialized to 0)
            store = ModbusSlaveContext(
                di=ModbusSequentialDataBlock(0, [0] * 65536),  # Discrete Inputs
                co=ModbusSequentialDataBlock(0, [0] * 65536),  # Coils
                hr=ModbusSequentialDataBlock(0, [0] * 65536),  # Holding Registers
                ir=ModbusSequentialDataBlock(0, [0] * 65536)   # Input Registers
            )
            
            self.context = ModbusServerContext(slaves=store, single=True)
            
            # Setup device identification
            identity = ModbusDeviceIdentification()
            identity.VendorName = 'OPC UA Gateway'
            identity.ProductCode = 'OPCUA-MB'
            identity.VendorUrl = 'https://github.com/yourrepo'
            identity.ProductName = 'OPC UA to MODBUS Bridge'
            identity.ModelName = 'MODBUS TCP Server'
            identity.MajorMinorRevision = '1.0.0'
            
            # Start server in separate thread
            def run_server():
                StartTcpServer(
                    context=self.context,
                    identity=identity,
                    address=(host, port),
                    allow_reuse_address=True
                )
            
            self.server_thread = threading.Thread(target=run_server, daemon=True)
            self.server_thread.start()
            self.running = True
            
            self.logger.info(f"MODBUS TCP server started on {host}:{port}")
            self.logger.info(f"Allocated {self.next_register} MODBUS registers")
            
        except Exception as e:
            self.logger.error(f"Failed to start MODBUS TCP publisher: {e}")
            self.running = False
    
    def stop(self):
        """Stop the MODBUS TCP server."""
        if self.running:
            # pymodbus server doesn't have clean shutdown in thread mode
            # Server will stop when daemon thread terminates
            self.running = False
            self.logger.info("MODBUS TCP publisher stopped")
    
    def value_to_registers(self, value: Any, tag_type: str) -> list:
        """
        Convert a value to MODBUS register values.
        
        Args:
            value: The value to convert
            tag_type: Type of the value
            
        Returns:
            List of register values (16-bit integers)
        """
        if tag_type == "float":
            # Convert float to 2 registers (32-bit IEEE 754)
            packed = struct.pack('>f', float(value))
            reg1, reg2 = struct.unpack('>HH', packed)
            return [reg1, reg2]
        
        elif tag_type == "int":
            # Convert int to 1 register (signed 16-bit)
            # Handle values outside 16-bit range
            int_value = int(value)
            if int_value > 32767:
                int_value = 32767
            elif int_value < -32768:
                int_value = -32768
            # Convert to unsigned for register storage
            reg_value = int_value & 0xFFFF
            return [reg_value]
        
        elif tag_type == "bool":
            # Convert bool to 1 register (0 or 1)
            return [1 if value else 0]
        
        elif tag_type == "string":
            # Convert string to registers (2 chars per register)
            str_value = str(value)[:64]  # Limit to 64 chars
            registers = []
            for i in range(0, len(str_value), 2):
                char1 = ord(str_value[i]) if i < len(str_value) else 0
                char2 = ord(str_value[i + 1]) if i + 1 < len(str_value) else 0
                registers.append((char1 << 8) | char2)
            # Pad to 32 registers
            while len(registers) < 32:
                registers.append(0)
            return registers
        
        return [0]
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Update MODBUS registers with tag value.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp (not used in MODBUS)
        """
        if not self.enabled or not self.running or not MODBUS_AVAILABLE:
            return
        
        try:
            # Get or allocate registers for this tag
            if tag_name not in self.tag_register_map:
                # Auto-allocate if not in mapping
                tag_type = "float"  # Default to float
                if isinstance(value, bool):
                    tag_type = "bool"
                elif isinstance(value, int):
                    tag_type = "int"
                elif isinstance(value, str):
                    tag_type = "string"
                
                self.allocate_registers(tag_name, tag_type)
            
            tag_info = self.tag_register_map[tag_name]
            start_register = tag_info["start_register"]
            tag_type = tag_info["type"]
            
            # Convert value to register values
            register_values = self.value_to_registers(value, tag_type)
            
            # Update holding registers in the datastore
            slave_context = self.context[0]
            for i, reg_value in enumerate(register_values):
                slave_context.setValues(3, start_register + i, [reg_value])  # Function code 3 = holding registers
            
            self.logger.debug(f"Updated MODBUS registers {start_register}-{start_register + len(register_values) - 1}: {tag_name} = {value}")
            
        except Exception as e:
            self.logger.error(f"Error publishing to MODBUS: {e}")
    
    def get_register_map(self) -> Dict[str, Any]:
        """
        Get the current register mapping for documentation.
        
        Returns:
            Dictionary mapping tag names to register info
        """
        return self.tag_register_map.copy()


class AlarmsPublisher(DataPublisher):
    """
    Alarms & Notifications Publisher - Because sometimes things go wrong
    
    Monitors tag values against configurable thresholds and sends alerts via:
    - Email (SMTP)
    - Slack (Webhooks)
    - SMS (Twilio)
    - Logging (always enabled)
    
    Features:
    - Threshold-based alerting (>, <, ==, !=)
    - Priority levels (INFO, WARNING, CRITICAL)
    - Debouncing (avoid spam)
    - Alarm history
    - Multiple notification channels
    - Auto-clear when values return to normal
    
    Because at 3 AM, you want to know if the reactor temperature is getting spicy.
    """
    
    PRIORITY_INFO = "INFO"
    PRIORITY_WARNING = "WARNING"
    PRIORITY_CRITICAL = "CRITICAL"
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialize the alarms publisher.
        
        Config structure:
        {
            "enabled": true,
            "rules": [
                {
                    "name": "High Temperature",
                    "tag": "Temperature",
                    "condition": ">",
                    "threshold": 25.0,
                    "priority": "CRITICAL",
                    "debounce_seconds": 60,
                    "message": "Temperature is too high!"
                }
            ],
            "notifications": {
                "email": {...},
                "slack": {...},
                "sms": {...}
            },
            "history_size": 1000
        }
        """
        super().__init__(config, logger)
        
        self.rules = config.get("rules", [])
        self.notifications_config = config.get("notifications", {})
        self.history_size = config.get("history_size", 1000)
        
        # Active alarms tracking
        self.active_alarms = {}  # tag_name -> alarm_info
        self.alarm_history = deque(maxlen=self.history_size)
        
        # Last notification time per rule (for debouncing)
        self.last_notification = {}  # rule_name -> timestamp
        
        # Parse rules
        self.parsed_rules = []
        for rule in self.rules:
            self.parsed_rules.append({
                "name": rule.get("name", "Unnamed Rule"),
                "tag": rule.get("tag"),
                "condition": rule.get("condition", ">"),
                "threshold": rule.get("threshold"),
                "priority": rule.get("priority", self.PRIORITY_WARNING),
                "debounce_seconds": rule.get("debounce_seconds", 60),
                "message": rule.get("message", "Alarm triggered"),
                "auto_clear": rule.get("auto_clear", True),
                "channels": rule.get("channels", ["log"])  # log, email, slack, sms
            })
        
        self.logger.info(f"Alarms publisher initialized with {len(self.parsed_rules)} rules")
    
    def start(self):
        """Start the alarms publisher."""
        if not self.enabled:
            self.logger.info("Alarms publisher is disabled")
            return
        
        self.running = True
        self.logger.info(f"Alarms publisher started - monitoring {len(self.parsed_rules)} rules")
    
    def stop(self):
        """Stop the alarms publisher."""
        self.running = False
        self.logger.info("Alarms publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Check tag value against alarm rules.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not self.running:
            return
        
        # Check each rule that applies to this tag
        for rule in self.parsed_rules:
            if rule["tag"] != tag_name:
                continue
            
            # Evaluate condition
            triggered = self._evaluate_condition(value, rule["condition"], rule["threshold"])
            
            rule_key = f"{rule['name']}_{tag_name}"
            
            if triggered:
                # Alarm condition met
                if rule_key not in self.active_alarms:
                    # New alarm
                    self._trigger_alarm(rule, tag_name, value, timestamp)
                else:
                    # Alarm already active, just update value
                    self.active_alarms[rule_key]["last_value"] = value
                    self.active_alarms[rule_key]["last_update"] = timestamp or time.time()
            else:
                # Alarm condition not met
                if rule_key in self.active_alarms and rule["auto_clear"]:
                    # Clear alarm
                    self._clear_alarm(rule, tag_name, value, timestamp)
    
    def _evaluate_condition(self, value: Any, condition: str, threshold: Any) -> bool:
        """Evaluate if value meets alarm condition."""
        try:
            if condition == ">":
                return float(value) > float(threshold)
            elif condition == ">=":
                return float(value) >= float(threshold)
            elif condition == "<":
                return float(value) < float(threshold)
            elif condition == "<=":
                return float(value) <= float(threshold)
            elif condition == "==":
                return value == threshold
            elif condition == "!=":
                return value != threshold
            else:
                self.logger.warning(f"Unknown condition: {condition}")
                return False
        except (ValueError, TypeError) as e:
            self.logger.error(f"Error evaluating condition: {e}")
            return False
    
    def _trigger_alarm(self, rule: Dict, tag_name: str, value: Any, timestamp: Optional[float]):
        """Trigger a new alarm."""
        rule_key = f"{rule['name']}_{tag_name}"
        now = timestamp or time.time()
        
        # Check debounce
        if rule_key in self.last_notification:
            time_since_last = now - self.last_notification[rule_key]
            if time_since_last < rule["debounce_seconds"]:
                self.logger.debug(f"Alarm {rule_key} debounced ({time_since_last:.1f}s < {rule['debounce_seconds']}s)")
                return
        
        # Create alarm record
        alarm = {
            "rule_name": rule["name"],
            "tag": tag_name,
            "priority": rule["priority"],
            "message": rule["message"],
            "condition": f"{rule['condition']} {rule['threshold']}",
            "triggered_value": value,
            "last_value": value,
            "triggered_at": now,
            "last_update": now,
            "cleared_at": None,
            "status": "ACTIVE"
        }
        
        self.active_alarms[rule_key] = alarm
        self.alarm_history.append(alarm.copy())
        
        # Send notifications
        self._send_notifications(alarm, rule["channels"])
        
        self.last_notification[rule_key] = now
        
        # Log
        priority_emoji = {"INFO": "â„¹ï¸", "WARNING": "âš ï¸", "CRITICAL": "ðŸš¨"}
        emoji = priority_emoji.get(rule["priority"], "âš ï¸")
        self.logger.warning(
            f"{emoji} ALARM TRIGGERED: {rule['name']} - {tag_name}={value} {rule['condition']} {rule['threshold']} - {rule['message']}"
        )
    
    def _clear_alarm(self, rule: Dict, tag_name: str, value: Any, timestamp: Optional[float]):
        """Clear an active alarm."""
        rule_key = f"{rule['name']}_{tag_name}"
        now = timestamp or time.time()
        
        alarm = self.active_alarms[rule_key]
        alarm["status"] = "CLEARED"
        alarm["cleared_at"] = now
        alarm["cleared_value"] = value
        
        # Update history
        self.alarm_history.append(alarm.copy())
        
        # Remove from active
        del self.active_alarms[rule_key]
        
        # Log
        self.logger.info(
            f"âœ… ALARM CLEARED: {rule['name']} - {tag_name}={value} (was {alarm['triggered_value']})"
        )
        
        # Optionally send clear notification
        if "clear" in rule.get("channels", []):
            clear_alarm = alarm.copy()
            clear_alarm["message"] = f"CLEARED: {rule['message']}"
            self._send_notifications(clear_alarm, rule["channels"])
    
    def _send_notifications(self, alarm: Dict, channels: list):
        """Send alarm notifications to configured channels."""
        for channel in channels:
            if channel == "log":
                # Already logged
                pass
            elif channel == "email":
                self._send_email(alarm)
            elif channel == "slack":
                self._send_slack(alarm)
            elif channel == "sms":
                self._send_sms(alarm)
    
    def _send_email(self, alarm: Dict):
        """Send email notification."""
        email_config = self.notifications_config.get("email", {})
        if not email_config.get("enabled", False):
            return
        
        try:
            smtp_server = email_config.get("smtp_server", "localhost")
            smtp_port = email_config.get("smtp_port", 587)
            username = email_config.get("username", "")
            password = email_config.get("password", "")
            from_addr = email_config.get("from", "opcua@fireball.local")
            to_addrs = email_config.get("to", [])
            
            if not to_addrs:
                return
            
            # Create message
            msg = MIMEMultipart()
            msg["From"] = from_addr
            msg["To"] = ", ".join(to_addrs)
            msg["Subject"] = f"[{alarm['priority']}] {alarm['rule_name']} - {alarm['tag']}"
            
            body = f"""
            Alarm: {alarm['rule_name']}
            Priority: {alarm['priority']}
            Tag: {alarm['tag']}
            Value: {alarm['triggered_value']}
            Condition: {alarm['condition']}
            Message: {alarm['message']}
            Time: {datetime.fromtimestamp(alarm['triggered_at']).strftime('%Y-%m-%d %H:%M:%S')}
            Status: {alarm['status']}
            """
            
            msg.attach(MIMEText(body, "plain"))
            
            # Send
            with smtplib.SMTP(smtp_server, smtp_port) as server:
                if username and password:
                    server.starttls()
                    server.login(username, password)
                server.send_message(msg)
            
            self.logger.info(f"Email notification sent for alarm: {alarm['rule_name']}")
            
        except Exception as e:
            self.logger.error(f"Error sending email notification: {e}")
    
    def _send_slack(self, alarm: Dict):
        """Send Slack notification via webhook."""
        slack_config = self.notifications_config.get("slack", {})
        if not slack_config.get("enabled", False):
            return
        
        try:
            webhook_url = slack_config.get("webhook_url")
            if not webhook_url:
                return
            
            # Priority colors
            color_map = {
                "INFO": "#36a64f",
                "WARNING": "#ff9900",
                "CRITICAL": "#ff0000"
            }
            
            # Create Slack message
            payload = {
                "attachments": [
                    {
                        "color": color_map.get(alarm["priority"], "#808080"),
                        "title": f"[{alarm['priority']}] {alarm['rule_name']}",
                        "text": alarm["message"],
                        "fields": [
                            {"title": "Tag", "value": alarm["tag"], "short": True},
                            {"title": "Value", "value": str(alarm["triggered_value"]), "short": True},
                            {"title": "Condition", "value": alarm["condition"], "short": True},
                            {"title": "Status", "value": alarm["status"], "short": True}
                        ],
                        "footer": "OPC UA Alarm System",
                        "ts": int(alarm["triggered_at"])
                    }
                ]
            }
            
            response = requests.post(webhook_url, json=payload, timeout=5)
            response.raise_for_status()
            
            self.logger.info(f"Slack notification sent for alarm: {alarm['rule_name']}")
            
        except Exception as e:
            self.logger.error(f"Error sending Slack notification: {e}")
    
    def _send_sms(self, alarm: Dict):
        """Send SMS notification via Twilio."""
        if not TWILIO_AVAILABLE:
            self.logger.warning("Twilio library not available for SMS notifications")
            return
        
        sms_config = self.notifications_config.get("sms", {})
        if not sms_config.get("enabled", False):
            return
        
        try:
            account_sid = sms_config.get("account_sid")
            auth_token = sms_config.get("auth_token")
            from_number = sms_config.get("from_number")
            to_numbers = sms_config.get("to_numbers", [])
            
            if not all([account_sid, auth_token, from_number, to_numbers]):
                return
            
            client = TwilioClient(account_sid, auth_token)
            
            message_body = f"[{alarm['priority']}] {alarm['rule_name']}: {alarm['tag']}={alarm['triggered_value']} {alarm['condition']}. {alarm['message']}"
            
            for to_number in to_numbers:
                client.messages.create(
                    body=message_body,
                    from_=from_number,
                    to=to_number
                )
            
            self.logger.info(f"SMS notification sent for alarm: {alarm['rule_name']}")
            
        except Exception as e:
            self.logger.error(f"Error sending SMS notification: {e}")
    
    def get_active_alarms(self) -> list:
        """Get list of currently active alarms."""
        return list(self.active_alarms.values())
    
    def get_alarm_history(self, limit: int = 100) -> list:
        """Get alarm history."""
        history_list = list(self.alarm_history)
        return history_list[-limit:] if limit else history_list
    
    def acknowledge_alarm(self, rule_name: str, tag_name: str, user: str = "system"):
        """Acknowledge an active alarm (doesn't clear it, just marks as acknowledged)."""
        rule_key = f"{rule_name}_{tag_name}"
        if rule_key in self.active_alarms:
            self.active_alarms[rule_key]["acknowledged"] = True
            self.active_alarms[rule_key]["acknowledged_by"] = user
            self.active_alarms[rule_key]["acknowledged_at"] = time.time()
            self.logger.info(f"Alarm acknowledged by {user}: {rule_name} - {tag_name}")
            return True
        return False


class InfluxDBPublisher(DataPublisher):
    """
    InfluxDB Publisher - Time-series database storage
    
    Writes tag data to InfluxDB for:
    - Historical data storage
    - Trend analysis
    - Grafana dashboards
    - Long-term data retention
    - Analytics and reporting
    
    Because sometimes you need to know what happened last Tuesday at 3:47 PM.
    And because "if it's not in the database, it didn't happen."
    """
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialize the InfluxDB publisher.
        
        Config structure:
        {
            "enabled": true,
            "url": "http://localhost:8086",
            "token": "your-influx-token",
            "org": "fireball-industries",
            "bucket": "industrial-data",
            "measurement": "opcua_tags",
            "batch_size": 100,
            "flush_interval": 1000  // milliseconds
        }
        """
        super().__init__(config, logger)
        
        if not INFLUXDB_AVAILABLE:
            self.logger.warning("InfluxDB client library not available. Install with: pip install influxdb-client")
            self.enabled = False
            return
        
        self.url = config.get("url", "http://localhost:8086")
        self.token = config.get("token", "")
        self.org = config.get("org", "fireball-industries")
        self.bucket = config.get("bucket", "industrial-data")
        self.measurement = config.get("measurement", "opcua_tags")
        self.batch_size = config.get("batch_size", 100)
        self.flush_interval = config.get("flush_interval", 1000)
        
        self.client = None
        self.write_api = None
        
        # Additional tags to add to each point
        self.global_tags = config.get("tags", {})
        
    def start(self):
        """Start the InfluxDB publisher."""
        if not self.enabled or not INFLUXDB_AVAILABLE:
            if not INFLUXDB_AVAILABLE:
                self.logger.warning("InfluxDB publisher is disabled (library not available)")
            else:
                self.logger.info("InfluxDB publisher is disabled")
            return
        
        try:
            # Create InfluxDB client
            self.client = InfluxDBClient(
                url=self.url,
                token=self.token,
                org=self.org
            )
            
            # Create write API with batching
            self.write_api = self.client.write_api(
                write_options=SYNCHRONOUS
            )
            
            # Test connection by pinging
            health = self.client.health()
            if health.status == "pass":
                self.logger.info(f"InfluxDB publisher started: {self.url} -> {self.bucket}")
                self.running = True
            else:
                self.logger.error(f"InfluxDB health check failed: {health.message}")
                self.enabled = False
                
        except Exception as e:
            self.logger.error(f"Failed to start InfluxDB publisher: {e}")
            self.enabled = False
    
    def stop(self):
        """Stop the InfluxDB publisher."""
        if self.write_api:
            try:
                self.write_api.close()
                self.logger.debug("InfluxDB write API closed")
            except Exception as e:
                self.logger.error(f"Error closing InfluxDB write API: {e}")
        
        if self.client:
            try:
                self.client.close()
                self.logger.info("InfluxDB publisher stopped")
            except Exception as e:
                self.logger.error(f"Error closing InfluxDB client: {e}")
        
        self.running = False
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Write tag value to InfluxDB.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp (uses current time if not provided)
        """
        if not self.enabled or not self.running or not INFLUXDB_AVAILABLE:
            return
        
        try:
            # Create point
            point = Point(self.measurement)
            
            # Add tag name as a tag (for efficient querying)
            point.tag("tag", tag_name)
            
            # Add global tags
            for tag_key, tag_value in self.global_tags.items():
                point.tag(tag_key, tag_value)
            
            # Add value as field (type-specific)
            if isinstance(value, bool):
                point.field("value_bool", value)
                point.field("value", 1 if value else 0)  # Numeric representation
            elif isinstance(value, int):
                point.field("value_int", value)
                point.field("value", float(value))
            elif isinstance(value, float):
                point.field("value_float", value)
                point.field("value", value)
            elif isinstance(value, str):
                point.field("value_string", value)
                # Try to parse as number for graphing
                try:
                    point.field("value", float(value))
                except (ValueError, TypeError):
                    pass
            else:
                point.field("value_string", str(value))
            
            # Set timestamp
            if timestamp:
                # Convert to nanoseconds (InfluxDB native precision)
                point.time(int(timestamp * 1e9), WritePrecision.NS)
            
            # Write to InfluxDB
            self.write_api.write(bucket=self.bucket, org=self.org, record=point)
            
            self.logger.debug(f"Wrote to InfluxDB: {tag_name} = {value}")
            
        except Exception as e:
            self.logger.error(f"Error writing to InfluxDB: {e}")


class GraphQLPublisher(DataPublisher):
    """
    GraphQL API Publisher - Modern query interface
    
    Provides a GraphQL endpoint for querying tag data with advanced features:
    - Flexible queries (get specific fields you need)
    - Real-time subscriptions (future)
    - Typed schema with introspection
    - Query batching
    - Filtering and pagination
    
    GraphQL is like REST API but you're in control of exactly what data you get back.
    No more over-fetching, no more under-fetching. Just vibes.
    """
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialize the GraphQL publisher.
        
        Config structure:
        {
            "enabled": true,
            "host": "0.0.0.0",
            "port": 5002,
            "graphiql": true,  // Enable GraphiQL web interface
            "cors_enabled": true
        }
        """
        super().__init__(config, logger)
        
        if not GRAPHQL_AVAILABLE:
            self.logger.warning("GraphQL libraries not available. Install with: pip install graphene flask-graphql")
            self.enabled = False
            return
        
        self.app = Flask(__name__)
        if config.get("cors_enabled", True):
            CORS(self.app)
        
        self.tags_data = {}  # In-memory tag storage
        self.server_thread = None
        
        # Build GraphQL schema
        self._setup_schema()
        
        # Add GraphQL endpoint
        host = config.get("host", "0.0.0.0")
        port = config.get("port", 5002)
        graphiql = config.get("graphiql", True)
        
        self.app.add_url_rule(
            '/graphql',
            view_func=GraphQLView.as_view(
                'graphql',
                schema=self.schema,
                graphiql=graphiql  # Enable GraphiQL IDE
            )
        )
        
        self.logger.info(f"GraphQL publisher initialized on http://{host}:{port}/graphql")
        if graphiql:
            self.logger.info(f"GraphiQL IDE available at http://{host}:{port}/graphql")
    
    def _setup_schema(self):
        """Setup GraphQL schema with types and queries."""
        
        # Define Tag type
        class TagType(graphene.ObjectType):
            name = graphene.String(description="Tag name")
            value = graphene.Field(
                graphene.String,
                description="Tag value (can be string, float, int, or bool)"
            )
            type = graphene.String(description="Data type of the tag")
            timestamp = graphene.Float(description="Last update timestamp")
            
            def resolve_value(self, info):
                # Return value as string for generic handling
                return str(self.value) if self.value is not None else None
        
        # Define Statistics type
        class TagStatsType(graphene.ObjectType):
            count = graphene.Int(description="Total number of tags")
            tags = graphene.List(graphene.String, description="List of tag names")
        
        # Define Query type
        class Query(graphene.ObjectType):
            # Get single tag
            tag = graphene.Field(
                TagType,
                name=graphene.String(required=True, description="Tag name to query"),
                description="Query a single tag by name"
            )
            
            # Get all tags
            tags = graphene.List(
                TagType,
                filter=graphene.String(description="Filter tags by name pattern"),
                description="Query all tags, optionally filtered"
            )
            
            # Get tag statistics
            stats = graphene.Field(
                TagStatsType,
                description="Get statistics about available tags"
            )
            
            def resolve_tag(self, info, name):
                """Resolve single tag query."""
                if name in self.tags_data:
                    tag_data = self.tags_data[name]
                    return TagType(
                        name=name,
                        value=tag_data.get('value'),
                        type=tag_data.get('type', 'unknown'),
                        timestamp=tag_data.get('timestamp')
                    )
                return None
            
            def resolve_tags(self, info, filter=None):
                """Resolve all tags query with optional filtering."""
                tags = []
                for name, tag_data in self.tags_data.items():
                    # Apply filter if provided
                    if filter and filter.lower() not in name.lower():
                        continue
                    
                    tags.append(TagType(
                        name=name,
                        value=tag_data.get('value'),
                        type=tag_data.get('type', 'unknown'),
                        timestamp=tag_data.get('timestamp')
                    ))
                return tags
            
            def resolve_stats(self, info):
                """Resolve stats query."""
                return TagStatsType(
                    count=len(self.tags_data),
                    tags=list(self.tags_data.keys())
                )
        
        # Bind tags_data to Query class for resolvers
        Query.tags_data = self.tags_data
        
        # Create schema
        self.schema = graphene.Schema(query=Query)
    
    def start(self):
        """Start the GraphQL API server."""
        if not self.enabled or not GRAPHQL_AVAILABLE:
            if not GRAPHQL_AVAILABLE:
                self.logger.warning("GraphQL publisher is disabled (libraries not available)")
            else:
                self.logger.info("GraphQL publisher is disabled")
            return
        
        try:
            host = self.config.get("host", "0.0.0.0")
            port = self.config.get("port", 5002)
            
            def run_server():
                self.app.run(host=host, port=port, debug=False, use_reloader=False)
            
            self.server_thread = threading.Thread(target=run_server, daemon=True)
            self.server_thread.start()
            
            self.logger.info(f"GraphQL API server started on http://{host}:{port}/graphql")
            
        except Exception as e:
            self.logger.error(f"Failed to start GraphQL publisher: {e}")
    
    def stop(self):
        """Stop the GraphQL API server."""
        # Flask doesn't have a graceful shutdown in this mode
        # The daemon thread will stop when the main process stops
        self.logger.info("GraphQL publisher stopped")
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Update tag value in GraphQL data store.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        if not self.enabled or not GRAPHQL_AVAILABLE:
            return
        
        # Determine type
        value_type = type(value).__name__
        
        # Store tag data
        self.tags_data[tag_name] = {
            "value": value,
            "type": value_type,
            "timestamp": timestamp or time.time()
        }


class OPCUAClientPublisher(DataPublisher):
    """
    OPC UA Client Publisher - Push data to other OPC UA servers
    
    This publisher acts as an OPC UA client and writes tag values to
    nodes on remote OPC UA servers. Useful for pushing data to:
    - Ignition's OPC UA server
    - KEPServerEX
    - Other OPC UA servers in the network
    - Historian systems with OPC UA interfaces
    
    Features:
    - Auto-connect and reconnect on disconnection
    - Node browsing and creation
    - Multiple server support
    - Username/password authentication
    - Certificate-based security (if configured)
    - Automatic namespace handling
    """
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialize the OPC UA Client publisher.
        
        Config structure:
        {
            "enabled": true,
            "servers": [
                {
                    "url": "opc.tcp://remote-server:4840",
                    "name": "Ignition Server",
                    "username": "admin",  # Optional
                    "password": "password",  # Optional
                    "namespace": 2,  # Namespace index for creating nodes
                    "base_node": "ns=2;s=Gateway/",  # Base node path
                    "auto_create_nodes": true,  # Create nodes if they don't exist
                    "node_mapping": {  # Optional: map tag names to specific node IDs
                        "Temperature": "ns=2;s=Gateway/Temperature",
                        "Pressure": "ns=2;s=Gateway/Pressure"
                    }
                }
            ],
            "reconnect_interval": 5  # Seconds between reconnection attempts
        }
        """
        super().__init__(config, logger)
        
        if not OPCUA_CLIENT_AVAILABLE:
            self.logger.warning("OPC UA Client library not available. Install with: pip install opcua")
            self.enabled = False
            return
        
        self.servers_config = config.get("servers", [])
        self.reconnect_interval = config.get("reconnect_interval", 5)
        
        # Track client connections
        self.clients = {}  # server_name -> {"client": OPCUAClient, "connected": bool, "nodes": {}}
        self.running = False
        self.reconnect_thread = None
        
        self.logger.info(f"OPC UA Client publisher initialized with {len(self.servers_config)} server(s)")
    
    def start(self):
        """Start the OPC UA client publisher."""
        if not self.enabled:
            return
        
        self.running = True
        
        # Connect to all configured servers
        for server_config in self.servers_config:
            self._connect_to_server(server_config)
        
        # Start reconnection thread
        self.reconnect_thread = threading.Thread(target=self._reconnect_loop, daemon=True)
        self.reconnect_thread.start()
        
        self.logger.info("OPC UA Client publisher started")
    
    def stop(self):
        """Stop the OPC UA client publisher."""
        if not self.enabled:
            return
        
        self.running = False
        
        # Disconnect all clients
        for server_name, client_info in self.clients.items():
            try:
                if client_info["connected"]:
                    client_info["client"].disconnect()
                    self.logger.info(f"Disconnected from OPC UA server: {server_name}")
            except Exception as e:
                self.logger.error(f"Error disconnecting from {server_name}: {e}")
        
        self.clients.clear()
        self.logger.info("OPC UA Client publisher stopped")
    
    def _connect_to_server(self, server_config: Dict[str, Any]):
        """
        Connect to a single OPC UA server.
        
        Args:
            server_config: Server configuration dictionary
        """
        server_name = server_config.get("name", server_config["url"])
        url = server_config["url"]
        
        try:
            client = OPCUAClient(url)
            
            # Set authentication if provided
            if server_config.get("username") and server_config.get("password"):
                client.set_user(server_config["username"])
                client.set_password(server_config["password"])
            
            # Connect
            client.connect()
            
            # Store client info
            self.clients[server_name] = {
                "client": client,
                "connected": True,
                "config": server_config,
                "nodes": {},  # tag_name -> node object cache
                "root": client.get_root_node(),
                "objects": client.get_objects_node()
            }
            
            self.logger.info(f"Connected to OPC UA server: {server_name} ({url})")
            
        except Exception as e:
            self.logger.error(f"Failed to connect to {server_name}: {e}")
            self.clients[server_name] = {
                "client": None,
                "connected": False,
                "config": server_config,
                "nodes": {}
            }
    
    def _reconnect_loop(self):
        """Background thread to reconnect to disconnected servers."""
        while self.running:
            time.sleep(self.reconnect_interval)
            
            for server_name, client_info in list(self.clients.items()):
                if not client_info["connected"]:
                    self.logger.info(f"Attempting to reconnect to {server_name}...")
                    self._connect_to_server(client_info["config"])
    
    def _get_or_create_node(self, client_info: Dict[str, Any], tag_name: str):
        """
        Get or create an OPC UA node for a tag.
        
        Args:
            client_info: Client information dictionary
            tag_name: Tag name
            
        Returns:
            Node object or None
        """
        # Check cache first
        if tag_name in client_info["nodes"]:
            return client_info["nodes"][tag_name]
        
        config = client_info["config"]
        client = client_info["client"]
        
        # Check for explicit node mapping
        node_mapping = config.get("node_mapping", {})
        if tag_name in node_mapping:
            node_id = node_mapping[tag_name]
            try:
                node = client.get_node(node_id)
                client_info["nodes"][tag_name] = node
                return node
            except Exception as e:
                self.logger.error(f"Failed to get mapped node {node_id}: {e}")
                return None
        
        # Build node path from base_node + tag_name
        base_node = config.get("base_node", "")
        if base_node:
            node_id = f"{base_node}{tag_name}"
        else:
            namespace = config.get("namespace", 2)
            node_id = f"ns={namespace};s={tag_name}"
        
        try:
            # Try to get existing node
            node = client.get_node(node_id)
            # Verify it exists by reading a value (will throw if doesn't exist)
            _ = node.get_browse_name()
            client_info["nodes"][tag_name] = node
            return node
        except Exception:
            # Node doesn't exist
            if config.get("auto_create_nodes", False):
                try:
                    # Create a new variable node
                    objects = client_info["objects"]
                    namespace = config.get("namespace", 2)
                    
                    # Create the variable
                    node = objects.add_variable(namespace, tag_name, 0.0)
                    node.set_writable()
                    
                    client_info["nodes"][tag_name] = node
                    self.logger.info(f"Created new node: {node_id}")
                    return node
                except Exception as e:
                    self.logger.error(f"Failed to create node {node_id}: {e}")
                    return None
            else:
                self.logger.warning(f"Node {node_id} not found and auto_create_nodes is disabled")
                return None
    
    def publish(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish tag value to all connected OPC UA servers.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp (currently not used)
        """
        if not self.enabled:
            return
        
        for server_name, client_info in self.clients.items():
            if not client_info["connected"]:
                continue
            
            try:
                # Get or create the node
                node = self._get_or_create_node(client_info, tag_name)
                if not node:
                    continue
                
                # Convert Python types to OPC UA DataValue
                if isinstance(value, bool):
                    ua_value = ua.DataValue(ua.Variant(value, ua.VariantType.Boolean))
                elif isinstance(value, int):
                    ua_value = ua.DataValue(ua.Variant(value, ua.VariantType.Int32))
                elif isinstance(value, float):
                    ua_value = ua.DataValue(ua.Variant(value, ua.VariantType.Double))
                elif isinstance(value, str):
                    ua_value = ua.DataValue(ua.Variant(value, ua.VariantType.String))
                else:
                    ua_value = ua.DataValue(ua.Variant(str(value), ua.VariantType.String))
                
                # Write the value
                node.set_value(ua_value)
                self.logger.debug(f"Wrote {tag_name}={value} to {server_name}")
                
            except Exception as e:
                self.logger.error(f"Error writing {tag_name} to {server_name}: {e}")
                # Mark as disconnected on error
                client_info["connected"] = False


class PublisherManager:
    """Manages multiple data publishers."""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialize the publisher manager.
        
        Args:
            config: Configuration dictionary with publisher settings
            logger: Logger instance
        """
        self.config = config
        self.logger = logger or logging.getLogger("PublisherManager")
        self.publishers = []
        
    def initialize_publishers(self):
        """Initialize all configured publishers."""
        publishers_config = self.config.get("publishers", {})
        
        # MQTT Publisher
        mqtt_config = publishers_config.get("mqtt", {})
        if mqtt_config.get("enabled", False):
            mqtt_pub = MQTTPublisher(mqtt_config, self.logger)
            self.publishers.append(mqtt_pub)
            self.logger.info("MQTT publisher initialized")
        
        # Sparkplug B Publisher (for Ignition)
        sparkplug_config = publishers_config.get("sparkplug_b", {})
        if sparkplug_config.get("enabled", False):
            sparkplug_pub = SparkplugBPublisher(sparkplug_config, self.logger)
            self.publishers.append(sparkplug_pub)
            self.logger.info("Sparkplug B publisher initialized")
        
        # Kafka Publisher
        kafka_config = publishers_config.get("kafka", {})
        if kafka_config.get("enabled", False):
            kafka_pub = KafkaPublisher(kafka_config, self.logger)
            self.publishers.append(kafka_pub)
            self.logger.info("Kafka publisher initialized")
        
        # AMQP Publisher (RabbitMQ)
        amqp_config = publishers_config.get("amqp", {})
        if amqp_config.get("enabled", False):
            amqp_pub = AMQPPublisher(amqp_config, self.logger)
            self.publishers.append(amqp_pub)
            self.logger.info("AMQP publisher initialized")
        
        # WebSocket Publisher
        websocket_config = publishers_config.get("websocket", {})
        if websocket_config.get("enabled", False):
            websocket_pub = WebSocketPublisher(websocket_config, self.logger)
            self.publishers.append(websocket_pub)
            self.logger.info("WebSocket publisher initialized")
        
        # MODBUS TCP Publisher
        modbus_config = publishers_config.get("modbus_tcp", {})
        if modbus_config.get("enabled", False):
            modbus_pub = ModbusTCPPublisher(modbus_config, self.logger)
            self.publishers.append(modbus_pub)
            self.logger.info("MODBUS TCP publisher initialized")
        
        # GraphQL API Publisher
        graphql_config = publishers_config.get("graphql", {})
        if graphql_config.get("enabled", False):
            graphql_pub = GraphQLPublisher(graphql_config, self.logger)
            self.publishers.append(graphql_pub)
            self.logger.info("GraphQL API publisher initialized")
        
        # InfluxDB Publisher
        influxdb_config = publishers_config.get("influxdb", {})
        if influxdb_config.get("enabled", False):
            influxdb_pub = InfluxDBPublisher(influxdb_config, self.logger)
            self.publishers.append(influxdb_pub)
            self.logger.info("InfluxDB publisher initialized")
        
        # Alarms Publisher
        alarms_config = publishers_config.get("alarms", {})
        if alarms_config.get("enabled", False):
            alarms_pub = AlarmsPublisher(alarms_config, self.logger)
            self.publishers.append(alarms_pub)
            self.logger.info("Alarms publisher initialized")
        
        # OPC UA Client Publisher
        opcua_client_config = publishers_config.get("opcua_client", {})
        if opcua_client_config.get("enabled", False):
            opcua_client_pub = OPCUAClientPublisher(opcua_client_config, self.logger)
            self.publishers.append(opcua_client_pub)
            self.logger.info("OPC UA Client publisher initialized")
        
        # REST API Publisher
        rest_config = publishers_config.get("rest_api", {})
        if rest_config.get("enabled", False):
            rest_pub = RESTAPIPublisher(rest_config, self.logger)
            self.publishers.append(rest_pub)
            self.logger.info("REST API publisher initialized")
        
        return self.publishers
    
    def start_all(self):
        """Start all publishers."""
        for publisher in self.publishers:
            try:
                publisher.start()
            except Exception as e:
                self.logger.error(f"Error starting publisher {publisher.__class__.__name__}: {e}")
    
    def stop_all(self):
        """Stop all publishers."""
        for publisher in self.publishers:
            try:
                publisher.stop()
            except Exception as e:
                self.logger.error(f"Error stopping publisher {publisher.__class__.__name__}: {e}")
    
    def publish_to_all(self, tag_name: str, value: Any, timestamp: Optional[float] = None):
        """
        Publish tag value to all enabled publishers.
        
        Args:
            tag_name: Name of the tag
            value: Tag value
            timestamp: Optional timestamp
        """
        for publisher in self.publishers:
            try:
                publisher.publish(tag_name, value, timestamp)
            except Exception as e:
                self.logger.error(f"Error publishing to {publisher.__class__.__name__}: {e}")
